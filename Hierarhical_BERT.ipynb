{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0e10b6",
   "metadata": {},
   "source": [
    "## Демонстрация иерархического с энкодером BERT и плоского классификатора на основе BERT энкодера."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3072bcb",
   "metadata": {},
   "source": [
    "Импортируем необходимые библиотеки и модули, в том числе, модули мпровизированной HierarchicalLibrary, в которых содержатся необходимые для работы иерархического классификатора классы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d3a25f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from HierarchicalLibrary import Classifier, CategoryTree, TextProcessor\n",
    "from HierarchicalLibrary.Encoders import LdaEncoder, NavecEncoder, FasttextEncoder, BertEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "38679f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "# Method for increasing the weight of the first words of title\n",
    "def word_pyramid(string: str, min_n_words: int, max_n_words: int) -> list:\n",
    "    result = []\n",
    "    split = string.split(' ')\n",
    "    for i in range(min_n_words, max_n_words+1):\n",
    "        result += split[:i]\n",
    "    return ' '.join(result)\n",
    "\n",
    "def prepare_data(full_train_data: pd.DataFrame, seed: int, valid_size: int):\n",
    "    data_full = full_train_data.sample(frac=1, random_state=seed).copy()\n",
    "    data_full.drop(['rating', 'feedback_quantity'], axis=1, inplace=True)\n",
    "    data_full.title = data_full.title.astype('string')\n",
    "    data_full.short_description = data_full.short_description.astype('string')\n",
    "    data_full.fillna(value='', inplace=True)\n",
    "    data_full.name_value_characteristics = data_full.name_value_characteristics.astype('string')\n",
    "    data_full = data_full.assign(Document=[str(x) + ' ' + str(y) + ' ' + str(z) + ' ' + str(x) for x, y, z in zip(data_full['title'], data_full['short_description'], data_full['name_value_characteristics'])])\n",
    "    data_full.drop(['title', 'short_description', 'name_value_characteristics'], axis=1, inplace=True)\n",
    "    data_full.Document = data_full.Document.astype('string')\n",
    "\n",
    "    data = data_full[:-valid_size].reset_index(drop=True)\n",
    "    data_valid = data_full[-valid_size:].reset_index(drop=True)\n",
    "    return data, data_valid\n",
    "\n",
    "def set_seeds(seed: int):  \n",
    "    np.random.seed(seed)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e914af5",
   "metadata": {},
   "source": [
    "Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f64d533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tree_df = pd.read_csv('categories_tree.csv', index_col=0)\n",
    "full_train_data = pd.read_parquet('train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a37cc8",
   "metadata": {},
   "source": [
    "Подготавливаем полный, тренировочный и валидационный датасеты: перемешиваем данные в фрейме, удаляем колонки рейтинга и кол-ва отзывов, корректируем типы данных колонок, заполняем пропущенные значения, текст из колонок 'title', 'short_description' и 'name_value_characteristics' объединяем в колонку \"Document\", добавляем первые слова из колонки 'title', чтобы увеличить их вес."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ed4d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(SEED)\n",
    "data, data_valid = prepare_data(full_train_data, seed=SEED, valid_size=4000)\n",
    "#data = data[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4375a",
   "metadata": {},
   "source": [
    "Для ускорения расчетов, оставим только 50000 записей, иначе, считать будет долго."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7bc02e72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1181186</td>\n",
       "      <td>12350</td>\n",
       "      <td>Маска Masil для объёма волос 8ml /Корейская ко...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304936</td>\n",
       "      <td>12917</td>\n",
       "      <td>Силиконовый дорожный контейнер футляр чехол дл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>816714</td>\n",
       "      <td>14125</td>\n",
       "      <td>Тканевая маска для лица с муцином улитки, 100%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1437391</td>\n",
       "      <td>11574</td>\n",
       "      <td>Браслеты из бисера Браслеты из бисера.  Брасле...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1234938</td>\n",
       "      <td>12761</td>\n",
       "      <td>Бальзам HAUTE COUTURE LUXURY BLOND для блондир...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279447</th>\n",
       "      <td>564872</td>\n",
       "      <td>11635</td>\n",
       "      <td>Крем-баттер для рук и тела MS.NAILS, 250 мл Кр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279448</th>\n",
       "      <td>1002594</td>\n",
       "      <td>12476</td>\n",
       "      <td>Цепочка на шею, 50 см Красивые, легкие и очень...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279449</th>\n",
       "      <td>988538</td>\n",
       "      <td>12302</td>\n",
       "      <td>Обложка на паспорт кожаная Кожаная обложка на ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279450</th>\n",
       "      <td>1014080</td>\n",
       "      <td>13407</td>\n",
       "      <td>Открытка средняя двойная на татарском языке  Р...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279451</th>\n",
       "      <td>802043</td>\n",
       "      <td>11678</td>\n",
       "      <td>Брюки для беременных BABY BUM, серый Базовые л...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279452 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  category_id  \\\n",
       "0       1181186        12350   \n",
       "1        304936        12917   \n",
       "2        816714        14125   \n",
       "3       1437391        11574   \n",
       "4       1234938        12761   \n",
       "...         ...          ...   \n",
       "279447   564872        11635   \n",
       "279448  1002594        12476   \n",
       "279449   988538        12302   \n",
       "279450  1014080        13407   \n",
       "279451   802043        11678   \n",
       "\n",
       "                                                 Document  \n",
       "0       Маска Masil для объёма волос 8ml /Корейская ко...  \n",
       "1       Силиконовый дорожный контейнер футляр чехол дл...  \n",
       "2       Тканевая маска для лица с муцином улитки, 100%...  \n",
       "3       Браслеты из бисера Браслеты из бисера.  Брасле...  \n",
       "4       Бальзам HAUTE COUTURE LUXURY BLOND для блондир...  \n",
       "...                                                   ...  \n",
       "279447  Крем-баттер для рук и тела MS.NAILS, 250 мл Кр...  \n",
       "279448  Цепочка на шею, 50 см Красивые, легкие и очень...  \n",
       "279449  Обложка на паспорт кожаная Кожаная обложка на ...  \n",
       "279450  Открытка средняя двойная на татарском языке  Р...  \n",
       "279451  Брюки для беременных BABY BUM, серый Базовые л...  \n",
       "\n",
       "[279452 rows x 3 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b776fe0",
   "metadata": {},
   "source": [
    "### Text Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f9673",
   "metadata": {},
   "source": [
    "Инициализируем объект энкодера (это класс, который управляет расчетами векторов скрытых представлений текстов, \"эмбеддингов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b5f70743",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextProcessor(\n",
    "    add_stop_words=[',', '.', '', '|', ':', '\"', '/', ')', '(', 'a', 'х', '(:', '):', ':(', ':)', 'и']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca80ba2",
   "metadata": {},
   "source": [
    "Следующий код читает документы из датафрейма, выполняет токенизацию и лемматизацию средствами пакета natasha, затем, сохраняет леммы в собственную переменную Encoder.texts. Лемматизация выполняется достаточно долго, поэтому сохраняем данные на диск:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "554e02ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatize: 100%|██████████| 279452/279452 [1:56:28<00:00, 39.99it/s]  \n"
     ]
    }
   ],
   "source": [
    "text_processor.lemmatize_data(data, document_col='Document', id_col='id')\n",
    "text_processor.save_lemms_data('50000_set_lemm', directory='Hierarhical_BERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578cf0c",
   "metadata": {},
   "source": [
    "Загружаем леммы с диска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0436323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor.load_lemms_data('50000_set_lemm', directory='Hierarhical_BERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601df374",
   "metadata": {},
   "source": [
    "#### Энкодер на базе модели BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ce80dbaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./cointegrated/rubert-tiny2 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_encoder = BertEncoder()\n",
    "bert_encoder.load_model(\"cointegrated/rubert-tiny2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d440709",
   "metadata": {},
   "source": [
    "В случае необходимости, считаем и сохраняем матрицу снижения размерности эмбеддингов navec и bert:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "65886dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_encoder.calc_pca(texts=text_processor.texts, sample_size=10000)\n",
    "bert_encoder.save_pca('PCA_bert.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cc08e",
   "metadata": {},
   "source": [
    "Загружаем матрицу для понижения размерности navec эмбеддингов (понижение размерности выполнено для увеличения производительности, если есть желание отключить понижение размерности - можно просто не передавать navec энкодеру значение dim):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cdc3946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_encoder.load_pca('PCA_bert.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8298f49",
   "metadata": {},
   "source": [
    "Размерность эмбеддинга BERT:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f88267fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "312"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_encoder.transform([['foo']]).shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50fb6a0",
   "metadata": {},
   "source": [
    "#### Словарь"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94679a82",
   "metadata": {},
   "source": [
    "Используя встроенный метод энкодера, формируем словарь эмбеддингов товаров вида {good_id(int) : embedding(np.array)}. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b95741de",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders=[bert_encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3424aa75",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = text_processor.make_embeddings_dict(encoders=encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7e94c",
   "metadata": {},
   "source": [
    "Сохраняем словарь эмбеддингов при необходимости - загружаем сохранённый:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4887a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(Path(\".\").parent, 'Hierarhical_BERT', '50000_set_embs_dict.pickle')\n",
    "with open(path, 'wb') as f:\n",
    "    pickle.dump(embeddings_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4341cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(Path(\".\").parent, 'Hierarhical_BERT', '50000_set_embs_dict.pickle')\n",
    "with open(path, 'rb') as f:\n",
    "    embeddings_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ccb16",
   "metadata": {},
   "source": [
    "### Дерево каталога"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d07d857",
   "metadata": {},
   "source": [
    "Инициализируем дерево каталога - CategoryTree() - это класс, который хранит все узлы, необходимую информацию для обучения, а также реализует алгоритмы заполнения дерева, обхода при инференсе для определения категории товара. \n",
    "Добавляем узлы из таблицы categories_tree.csv, затем, добавляем товары из тренировочной выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a8afb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tree = CategoryTree()\n",
    "cat_tree.add_nodes_from_df(cat_tree_df, parent_id_col='parent_id', title_col='title')\n",
    "cat_tree.add_goods_from_df(data, category_id_col='category_id', good_id_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dda3fd",
   "metadata": {},
   "source": [
    "Записываем эмбеддинги в дерево каталогов (производится расчет эмбеддингов узлов как усреднённых эмбеддингов документов, попавших в каждый узел):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bf8b3a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tree.update_embeddings(embeddings_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933296a9",
   "metadata": {},
   "source": [
    "Примешиваем к эмбеддингам узлов эмбеддинги их собственных описаний."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0211f730",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cat_tree.mix_in_description_embs(lambda titles: text_processor.get_embeddings(titles, encoders=encoders), weight=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be436986",
   "metadata": {},
   "source": [
    "### Классификатор"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29aef372",
   "metadata": {},
   "source": [
    "Инициализируем объект классификатора - он управляет процессом получения вероятностей принадлежности товара к узлу (predict_proba). CatBoost больше не нужен - просто небудем его инициализировать.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "519716db",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier(tol=0.03, max_iter=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f037fe",
   "metadata": {},
   "source": [
    "Обучаем локальные веса (модель логистической регрессии в каждом из узлов дерева). Сохраняем дерево (так как считается очень долго). При необходимости - загружаем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "13553b09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3370/3370 [1:04:40<00:00,  1.15s/it]\n"
     ]
    }
   ],
   "source": [
    "cat_tree.fit_local_weights(classifier, embeddings_dict, C=0.05, reg_count_power=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b994502",
   "metadata": {},
   "source": [
    "Сохраняем, и при необходимости, загружаем дерево с рассчитанными весами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b961f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tree.save_tree('50000_set_tree.pickle', directory='Hierarhical_BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2204b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tree.load_tree('50000_set_tree.pickle', directory='Hierarhical_BERT')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847914e3",
   "metadata": {},
   "source": [
    "### Тестирование модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0775bb3",
   "metadata": {},
   "source": [
    "#### Тестирование на трейне"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da40d51",
   "metadata": {},
   "source": [
    "Формируем массив эмбеддингов для тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b8d6397",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_example = 0\n",
    "end_example = 3000\n",
    "train_documents = data.Document.tolist()[begin_example:end_example]\n",
    "train_target = data.category_id.tolist()[begin_example:end_example]\n",
    "embs = text_processor.get_embeddings(train_documents, encoders=encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21d0fe",
   "metadata": {},
   "source": [
    "Выполняем поиск категорий по каталогу для каждого тестового примера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3a07cf12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [00:12<00:00, 247.28it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_leafs = []\n",
    "for i in tqdm.tqdm(range(len(embs)), total=len(embs)):\n",
    "    pred_leafs.append(cat_tree.choose_leaf(classifier = classifier, good_embedding=embs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3672349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set hF1=0.729\n"
     ]
    }
   ],
   "source": [
    "print(f'Train set hF1={cat_tree.hF1_score(train_target, pred_leafs):.3f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459b643a",
   "metadata": {},
   "source": [
    "#### Тестирование на отложенной выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad7a60",
   "metadata": {},
   "source": [
    "Формируем массив эмбеддингов для тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6e962267",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_example = 0\n",
    "end_example = 4000\n",
    "valid_documents = data_valid.Document.tolist()[begin_example:end_example]\n",
    "valid_target = data_valid.category_id.tolist()[begin_example:end_example]\n",
    "embs_valid = text_processor.get_embeddings(valid_documents, encoders=encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b936e16",
   "metadata": {},
   "source": [
    "Выполняем поиск категорий по каталогу для каждого тестового примера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "44eeae03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:16<00:00, 245.99it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_leafs_valid = []\n",
    "for i in tqdm.tqdm(range(len(embs_valid)), total=len(embs_valid)):\n",
    "    pred_leafs_valid.append(cat_tree.choose_leaf(classifier = classifier, good_embedding=embs_valid[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0d72d101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation hF1=0.718\n"
     ]
    }
   ],
   "source": [
    "print(f'Validation hF1={cat_tree.hF1_score(valid_target, pred_leafs_valid):.3f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f2a31",
   "metadata": {},
   "source": [
    "В этом ноутбуке гиперпараметры и размер выборки выбраны такими, чтобы расчёты выполнялись относительно быстро. С хорошими гиперпараметрами, на полном размере выборки, удалось получить hF1=0.86, что значительно ниже бейзлайна. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced3ab0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa357f32",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "23a67d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KEdataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, \n",
    "                 embeddings_dict: dict = None,\n",
    "                 document_list: list = None,\n",
    "                 encoders = None,\n",
    "                 id_col: str = 'id', \n",
    "                 category_col: str = None, \n",
    "                 document_col = None,\n",
    "                 text_processor: object = None,\n",
    "                 mode: str = 'test',\n",
    "                 label_encoder: object = None,\n",
    "                 simple_lemms: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        if embeddings_dict:\n",
    "            self.X = torch.tensor(np.array(\n",
    "                data[id_col].apply(lambda good_id: embeddings_dict[good_id]).tolist()), dtype=torch.float32)\n",
    "        else:\n",
    "            self.X = torch.tensor(text_processor.get_embeddings(\n",
    "                data[document_col].tolist(), \n",
    "                encoders=encoders, \n",
    "                simple_lemms=simple_lemms), dtype=torch.float32)\n",
    "        \n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode not in ['train', 'val', 'test']:\n",
    "            print(f\"{self.mode} is not correct; correct modes: {['train', 'val', 'test']}\")\n",
    "            raise NameError\n",
    "            \n",
    "        if label_encoder:\n",
    "            self.label_encoder = label_encoder\n",
    "        else:\n",
    "            self.label_encoder = LabelEncoder()\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self.labels = data[category_col].tolist()\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            self.label_encoder.fit(self.labels)\n",
    "        elif self.mode == 'val':\n",
    "            self.labels = data[category_col].tolist()\n",
    "            self.label_encoder = label_encoder            \n",
    "        return\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        x = self.X[index]\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return x\n",
    "        else:\n",
    "            label = self.labels[index]\n",
    "            label_id = self.label_encoder.transform([label])\n",
    "            y = label_id.item()\n",
    "            return x, y\n",
    "    \n",
    "    @property\n",
    "    def dim(self):\n",
    "        return train_dataset.X.shape[1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "189c903a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KEdataset(data=data, \n",
    "                          embeddings_dict=embeddings_dict, \n",
    "                          id_col='id', \n",
    "                          category_col='category_id', \n",
    "                          mode='train')\n",
    "\n",
    "valid_dataset = KEdataset(data=data_valid, \n",
    "                          encoders=encoders, \n",
    "                          document_col='Document', \n",
    "                          category_col='category_id', \n",
    "                          mode='val', \n",
    "                          text_processor=text_processor,\n",
    "                          label_encoder=train_dataset.label_encoder, \n",
    "                          simple_lemms=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cd6a4ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fcnn(nn.Module):\n",
    "  \n",
    "    def __init__(self, emb_dim: int, n_classes: int):\n",
    "        super().__init__()\n",
    "        self.out = nn.Linear(emb_dim, n_classes)\n",
    "        return\n",
    "  \n",
    "    def forward(self, x):\n",
    "        logits = self.out(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e9b7f94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer, sheduler, device: str = 'cpu'):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_data = 0\n",
    "  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(torch.device(device))\n",
    "        labels = labels.to(torch.device(device))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        #target = F.one_hot(labels, 1231).float()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_data += inputs.size(0)\n",
    "    sheduler.step()\n",
    "    train_loss = running_loss / processed_data\n",
    "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def eval_epoch(model, val_loader, criterion, device: str = 'cpu'):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(torch.device(device))\n",
    "        labels = labels.to(torch.device(device))\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            #target = F.one_hot(labels, 1231).float()\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_size += inputs.size(0)\n",
    "    val_loss = running_loss / processed_size\n",
    "    val_acc = running_corrects.double() / processed_size\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    return val_loss, val_acc\n",
    "\n",
    "def train(train_dataset, \n",
    "          val_dataset, \n",
    "          model, epochs, \n",
    "          batch_size, \n",
    "          num_workers=0, \n",
    "          lr: float = 0.01, lr_mult: float = 0.1,\n",
    "          weight_decay=0.0):\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
    "\n",
    "    with tqdm.tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        gamma = lr_mult ** (2/epochs)\n",
    "        sheduler = lr_scheduler.StepLR(opt, step_size=2, gamma=gamma, verbose=True)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt, sheduler)\n",
    "            print(\"loss\", train_loss)\n",
    "            \n",
    "            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
    "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
    "            \n",
    "    return history\n",
    "\n",
    "def predict(model, test_loader, device: str = 'cpu'):\n",
    "    with torch.no_grad():\n",
    "        logits = []\n",
    "    \n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(torch.device(device))\n",
    "            model.eval()\n",
    "            outputs = model(inputs).cpu()\n",
    "            logits.append(outputs)\n",
    "            \n",
    "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354f4333",
   "metadata": {},
   "source": [
    "Инициализация сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "96788245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will classify :1231\n",
      "Fcnn(\n",
      "  (out): Linear(in_features=312, out_features=1231, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(data.category_id.values))\n",
    "simple_nn = Fcnn(n_classes=n_classes, \n",
    "                  emb_dim=train_dataset.dim).to(torch.device(\"cpu\"))\n",
    "\n",
    "print(\"we will classify :{}\".format(n_classes))\n",
    "print(simple_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6469baf4",
   "metadata": {},
   "source": [
    "Обучение сети:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "01182f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "Adjusting learning rate of group 0 to 1.0000e-02.\n",
      "loss 0.8535139397301361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   5%|▌         | 1/20 [01:08<21:44, 68.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 001 train_loss: 0.8535     val_loss 1.5638 train_acc 0.8181 val_acc 0.6623\n",
      "Adjusting learning rate of group 0 to 7.0423e-03.\n",
      "loss 0.7585561316502336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  10%|█         | 2/20 [02:15<20:20, 67.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 002 train_loss: 0.7586     val_loss 1.5524 train_acc 0.8320 val_acc 0.6573\n",
      "Adjusting learning rate of group 0 to 7.0423e-03.\n",
      "loss 0.7178720505878138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  15%|█▌        | 3/20 [03:23<19:09, 67.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 003 train_loss: 0.7179     val_loss 1.5441 train_acc 0.8399 val_acc 0.6545\n",
      "Adjusting learning rate of group 0 to 4.9593e-03.\n",
      "loss 0.6983156049519279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  20%|██        | 4/20 [04:29<17:53, 67.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 004 train_loss: 0.6983     val_loss 1.5429 train_acc 0.8427 val_acc 0.6567\n",
      "Adjusting learning rate of group 0 to 4.9593e-03.\n",
      "loss 0.6817098708134846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  25%|██▌       | 5/20 [05:37<16:49, 67.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 005 train_loss: 0.6817     val_loss 1.5564 train_acc 0.8459 val_acc 0.6520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  25%|██▌       | 5/20 [06:11<18:33, 74.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [81]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimple_nn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m18630\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_mult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.03\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_dataset, val_dataset, model, epochs, batch_size, num_workers, lr, lr_mult, weight_decay)\u001b[0m\n\u001b[1;32m     71\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 74\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mfit_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, train_loss)\n\u001b[1;32m     77\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m eval_epoch(model, val_loader, criterion)\n",
      "Input \u001b[0;32mIn [74]\u001b[0m, in \u001b[0;36mfit_epoch\u001b[0;34m(model, train_loader, criterion, optimizer, sheduler, device)\u001b[0m\n\u001b[1;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mdevice(device))\n\u001b[1;32m      9\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#target = F.one_hot(labels, 1231).float()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n",
      "File \u001b[0;32m~/anaconda3/envs/ARTMenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [73]\u001b[0m, in \u001b[0;36mFcnn.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m----> 9\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m~/anaconda3/envs/ARTMenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/ARTMenv/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = train(train_dataset, \n",
    "                 valid_dataset, \n",
    "                 model=simple_nn, \n",
    "                 epochs=20, \n",
    "                 batch_size=18630, \n",
    "                 num_workers=0, \n",
    "                 lr=0.01, lr_mult = 0.03, \n",
    "                 weight_decay=0.1) \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce04ce0",
   "metadata": {},
   "source": [
    "Тест:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3caa05c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = KEdataset(data=data_valid, \n",
    "                         encoders=encoders, \n",
    "                         document_col='Document', \n",
    "                         category_col='category_id', \n",
    "                         mode='test', \n",
    "                         text_processor=text_processor,\n",
    "                         label_encoder=train_dataset.label_encoder, \n",
    "                         simple_lemms=True)\n",
    "val_preds = predict(simple_nn, \n",
    "                    DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "70759d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation hF1=0.741\n"
     ]
    }
   ],
   "source": [
    "pred_torch_valid = list(train_dataset.label_encoder.inverse_transform(val_preds.argmax(axis=1)))\n",
    "print(f'Validation hF1={cat_tree.hF1_score(valid_target, pred_torch_valid):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016fe13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7539ff7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
