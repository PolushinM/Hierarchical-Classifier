{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0e10b6",
   "metadata": {},
   "source": [
    "## Демонстрация классификатора с энкодером FastText и иерархическим custom loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3072bcb",
   "metadata": {},
   "source": [
    "Импортируем необходимые библиотеки и модули, в том числе, модули мпровизированной HierarchicalLibrary, в которых содержатся необходимые для работы иерархического классификатора классы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a25f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import fasttext\n",
    "from gensim.utils import simple_preprocess\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from HierarchicalLibrary import Classifier, CategoryTree, TextProcessor\n",
    "from HierarchicalLibrary.Encoders import LdaEncoder, NavecEncoder, FasttextEncoder, BertEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "689fbf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "\n",
    "# Method for increasing the weight of the first words of title\n",
    "def word_pyramid(string: str, min_n_words: int, max_n_words: int) -> list:\n",
    "    result = []\n",
    "    split = string.split(' ')\n",
    "    for i in range(min_n_words, max_n_words+1):\n",
    "        result += split[:i]\n",
    "    return ' '.join(result)\n",
    "\n",
    "def prepare_data(full_train_data: pd.DataFrame, seed: int, valid_size: int):\n",
    "    data_full = full_train_data.sample(frac=1, random_state=seed).copy()\n",
    "    data_full.drop(['rating', 'feedback_quantity'], axis=1, inplace=True)\n",
    "    data_full.title = data_full.title.astype('string')\n",
    "    data_full.short_description = data_full.short_description.astype('string')\n",
    "    data_full.fillna(value='', inplace=True)\n",
    "    data_full.name_value_characteristics = data_full.name_value_characteristics.astype('string')\n",
    "    data_full = data_full.assign(Document=[str(x) + ' ' + str(y) + ' ' + str(z) + ' ' + word_pyramid(x, 2, 3) for x, y, z in zip(data_full['title'], data_full['short_description'], data_full['name_value_characteristics'])])\n",
    "    data_full.drop(['title', 'short_description', 'name_value_characteristics'], axis=1, inplace=True)\n",
    "    data_full.Document = data_full.Document.astype('string')\n",
    "\n",
    "    data = data_full[:-valid_size].reset_index(drop=True)\n",
    "    data_valid = data_full[-valid_size:].reset_index(drop=True)\n",
    "    return data, data_valid\n",
    "\n",
    "def set_seeds(seed: int):  \n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e914af5",
   "metadata": {},
   "source": [
    "Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f64d533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tree_df = pd.read_csv('categories_tree.csv', index_col=0)\n",
    "full_train_data = pd.read_parquet('train.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a37cc8",
   "metadata": {},
   "source": [
    "Подготавливаем полный, тренировочный и валидационный датасеты:\n",
    "перемешиваем данные в фрейме,\n",
    "удаляем колонки рейтинга и кол-ва отзывов,\n",
    "корректируем типы данных колонок,\n",
    "заполняем пропущенные значения,\n",
    "текст из колонок 'title', 'short_description' и 'name_value_characteristics' объединяем в колонку \"Document\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ed4d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(SEED)\n",
    "data, data_valid = prepare_data(full_train_data, seed=SEED, valid_size=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bc02e72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category_id</th>\n",
       "      <th>Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1181186</td>\n",
       "      <td>12350</td>\n",
       "      <td>Маска Masil для объёма волос 8ml /Корейская ко...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>304936</td>\n",
       "      <td>12917</td>\n",
       "      <td>Силиконовый дорожный контейнер футляр чехол дл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>816714</td>\n",
       "      <td>14125</td>\n",
       "      <td>Тканевая маска для лица с муцином улитки, 100%...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1437391</td>\n",
       "      <td>11574</td>\n",
       "      <td>Браслеты из бисера Браслеты из бисера.  Брасле...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1234938</td>\n",
       "      <td>12761</td>\n",
       "      <td>Бальзам HAUTE COUTURE LUXURY BLOND для блондир...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279447</th>\n",
       "      <td>564872</td>\n",
       "      <td>11635</td>\n",
       "      <td>Крем-баттер для рук и тела MS.NAILS, 250 мл Кр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279448</th>\n",
       "      <td>1002594</td>\n",
       "      <td>12476</td>\n",
       "      <td>Цепочка на шею, 50 см Красивые, легкие и очень...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279449</th>\n",
       "      <td>988538</td>\n",
       "      <td>12302</td>\n",
       "      <td>Обложка на паспорт кожаная Кожаная обложка на ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279450</th>\n",
       "      <td>1014080</td>\n",
       "      <td>13407</td>\n",
       "      <td>Открытка средняя двойная на татарском языке  Р...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279451</th>\n",
       "      <td>802043</td>\n",
       "      <td>11678</td>\n",
       "      <td>Брюки для беременных BABY BUM, серый Базовые л...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279452 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  category_id  \\\n",
       "0       1181186        12350   \n",
       "1        304936        12917   \n",
       "2        816714        14125   \n",
       "3       1437391        11574   \n",
       "4       1234938        12761   \n",
       "...         ...          ...   \n",
       "279447   564872        11635   \n",
       "279448  1002594        12476   \n",
       "279449   988538        12302   \n",
       "279450  1014080        13407   \n",
       "279451   802043        11678   \n",
       "\n",
       "                                                 Document  \n",
       "0       Маска Masil для объёма волос 8ml /Корейская ко...  \n",
       "1       Силиконовый дорожный контейнер футляр чехол дл...  \n",
       "2       Тканевая маска для лица с муцином улитки, 100%...  \n",
       "3       Браслеты из бисера Браслеты из бисера.  Брасле...  \n",
       "4       Бальзам HAUTE COUTURE LUXURY BLOND для блондир...  \n",
       "...                                                   ...  \n",
       "279447  Крем-баттер для рук и тела MS.NAILS, 250 мл Кр...  \n",
       "279448  Цепочка на шею, 50 см Красивые, легкие и очень...  \n",
       "279449  Обложка на паспорт кожаная Кожаная обложка на ...  \n",
       "279450  Открытка средняя двойная на татарском языке  Р...  \n",
       "279451  Брюки для беременных BABY BUM, серый Базовые л...  \n",
       "\n",
       "[279452 rows x 3 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b776fe0",
   "metadata": {},
   "source": [
    "### Энкодер"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f9673",
   "metadata": {},
   "source": [
    "Инициализируем объект процессора текста (это класс, который управляет лемматизацией и расчетами векторов скрытых представлений текстов, \"эмбеддингов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b5f70743",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor = TextProcessor(\n",
    "    add_stop_words=[',', '.', '', '|', ':', '\"', '/', ')', '(', 'a', 'х', '(:', '):', ':(', ':)', 'и']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca80ba2",
   "metadata": {},
   "source": [
    "Следующий код читает документы из датафрейма, выполняет токенизацию и лемматизацию средствами пакета natasha, затем, сохраняет леммы в собственную переменную TextProcessor.texts. \n",
    "Лемматизация выполняется достаточно долго, поэтому сохраняем данные на диск:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "004aa019",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lemmatize: 100%|██████████| 279452/279452 [00:15<00:00, 18034.98it/s]\n"
     ]
    }
   ],
   "source": [
    "text_processor.simple_lemmatize_data(data, document_col='Document', id_col='id')\n",
    "text_processor.save_lemms_data('full_train_data_s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7578cf0c",
   "metadata": {},
   "source": [
    "Загружаем леммы с диска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0436323f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_processor.load_lemms_data('full_train_data_s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22620ecd",
   "metadata": {},
   "source": [
    "Загружаем обученную модель fasttext (обучена отдельно)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7b3453c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "fasttext_encoder = FasttextEncoder()\n",
    "fasttext_encoder.load_model('fasttext_model_45_s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95fe9e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_encoder.transform([['foo']]).shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94679a82",
   "metadata": {},
   "source": [
    "Используя встроенный метод энкодера, формируем словарь эмбеддингов товаров вида {good_id(int) : embedding(np.array)}. Передаем интересующие нас энкодеры. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b95741de",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoders=[fasttext_encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3424aa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding: 0\n"
     ]
    }
   ],
   "source": [
    "embeddings_dict = text_processor.make_embeddings_dict(encoders=encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bf82d0",
   "metadata": {},
   "source": [
    "Проверяем полную размерность составного эмбеддинга:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a92c9186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_dict[next(iter(embeddings_dict))].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b7e94c",
   "metadata": {},
   "source": [
    "Сохраняем словарь эмбеддингов при необходимости - загружаем сохранённый:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0aaadd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = os.path.join(Path(\".\").parent, 'Hierarhical_no_catboost', '50000_set_embs_dict.pickle')\n",
    "with open('embs_dict_all_enc.pickle', 'wb') as f:\n",
    "    pickle.dump(embeddings_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9f579a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = os.path.join(Path(\".\").parent, 'Hierarhical_no_catboost', '50000_set_embs_dict.pickle')\n",
    "with open('embs_dict_all_enc.pickle', 'rb') as f:\n",
    "    embeddings_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ccb16",
   "metadata": {},
   "source": [
    "### Дерево каталога"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d07d857",
   "metadata": {},
   "source": [
    "Инициализируем дерево каталога - CategoryTree() - это класс, который хранит все узлы, необходимую информацию для обучения, а также реализует алгоритмы заполнения дерева, обхода при инференсе для определения категории товара. \n",
    "Добавляем узлы из таблицы categories_tree.csv, затем, добавляем товары из тренировочной выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a8afb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tree = CategoryTree()\n",
    "cat_tree.add_nodes_from_df(cat_tree_df, parent_id_col='parent_id', title_col='title')\n",
    "cat_tree.add_goods_from_df(data, category_id_col='category_id', good_id_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad7a60",
   "metadata": {},
   "source": [
    "Формируем массив эмбеддингов для тестирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e962267",
   "metadata": {},
   "outputs": [],
   "source": [
    "begin_example = 0\n",
    "end_example = 4000\n",
    "valid_documents = data_valid.Document.tolist()[begin_example:end_example]\n",
    "valid_target = data_valid.category_id.tolist()[begin_example:end_example]\n",
    "embs_valid = text_processor.get_embeddings(valid_documents, encoders=encoders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4823a49",
   "metadata": {},
   "source": [
    "## PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ce8e5dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KEdataset(Dataset):\n",
    "    def __init__(self, data: pd.DataFrame, \n",
    "                 embeddings_dict: dict = None,\n",
    "                 document_list: list = None,\n",
    "                 encoders = None,\n",
    "                 id_col: str = 'id', \n",
    "                 category_col: str = None, \n",
    "                 document_col = None,\n",
    "                 text_processor: object = None,\n",
    "                 mode: str = 'test',\n",
    "                 label_encoder: object = None,\n",
    "                 simple_lemms: bool = False) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        if embeddings_dict:\n",
    "            self.X = torch.tensor(np.array(\n",
    "                data[id_col].apply(lambda good_id: embeddings_dict[good_id]).tolist()), dtype=torch.float32)\n",
    "        else:\n",
    "            self.X = torch.tensor(text_processor.get_embeddings(\n",
    "                data[document_col].tolist(), \n",
    "                encoders=encoders, \n",
    "                simple_lemms=simple_lemms), dtype=torch.float32)\n",
    "        \n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode not in ['train', 'val', 'test']:\n",
    "            print(f\"{self.mode} is not correct; correct modes: {['train', 'val', 'test']}\")\n",
    "            raise NameError\n",
    "            \n",
    "        if label_encoder:\n",
    "            self.label_encoder = label_encoder\n",
    "        else:\n",
    "            self.label_encoder = LabelEncoder()\n",
    "\n",
    "        if self.mode == 'train':\n",
    "            self.labels = data[category_col].tolist()\n",
    "            self.label_encoder = LabelEncoder()\n",
    "            self.label_encoder.fit(self.labels)\n",
    "        elif self.mode == 'val':\n",
    "            self.labels = data[category_col].tolist()\n",
    "            self.label_encoder = label_encoder            \n",
    "        return\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        x = self.X[index]\n",
    "\n",
    "        if self.mode == 'test':\n",
    "            return x\n",
    "        else:\n",
    "            label = self.labels[index]\n",
    "            label_id = self.label_encoder.transform([label])\n",
    "            y = label_id.item()\n",
    "            return x, y\n",
    "    \n",
    "    @property\n",
    "    def dim(self):\n",
    "        return train_dataset.X.shape[1]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9908947",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KEdataset(data=data, \n",
    "                          embeddings_dict=embeddings_dict, \n",
    "                          id_col='id', \n",
    "                          category_col='category_id', \n",
    "                          mode='train')\n",
    "\n",
    "valid_dataset = KEdataset(data=data_valid, \n",
    "                          encoders=encoders, \n",
    "                          document_col='Document', \n",
    "                          category_col='category_id', \n",
    "                          mode='val', \n",
    "                          text_processor=text_processor,\n",
    "                          label_encoder=train_dataset.label_encoder, \n",
    "                          simple_lemms=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8133d6c4",
   "metadata": {},
   "source": [
    "Один слой нейросети (аналогично тому, что используется в fasttext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60db9ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fcnn(nn.Module):\n",
    "  \n",
    "    def __init__(self, emb_dim: int, hidden_dim: int, n_classes: int, dropout: float = 0.3):\n",
    "        super().__init__()\n",
    "        self.out = nn.Linear(emb_dim, n_classes)\n",
    "        return\n",
    "  \n",
    "    def forward(self, x):\n",
    "        logits = self.out(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd05d45",
   "metadata": {},
   "source": [
    "Рассчитываем матрицу расстояний между листьями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "a7560a3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_distance_matrix(cat_labels: np.array, encoder: object, power: float = 1.0) -> np.array:\n",
    "    encode_labels = np.vstack((encoder.transform(cat_labels), cat_labels)).T\n",
    "    distance_matrix = np.zeros((encode_labels.shape[0], encode_labels.shape[0]))\n",
    "\n",
    "    for enc_label_1, label_1 in encode_labels:\n",
    "        for enc_label_2, label_2 in encode_labels:\n",
    "            path_1 = cat_tree.get_id_path_set(label_1)\n",
    "            path_2 = cat_tree.get_id_path_set(label_2)\n",
    "            intersect = path_1.intersection(path_2)\n",
    "            value = (len(path_1)+len(path_1))/2 - len(intersect) + 1\n",
    "            distance_matrix[enc_label_1][enc_label_2] = value\n",
    "    \n",
    "    mean_value = distance_matrix.mean()\n",
    "\n",
    "    for enc_label, label in encode_labels:\n",
    "        distance_matrix[enc_label][enc_label] = mean_value\n",
    "        \n",
    "    #distance_matrix = np.log(distance_matrix)\n",
    "    return torch.tensor(distance_matrix / distance_matrix.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "563685d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_labels = data.category_id.value_counts().index.values\n",
    "distance_matrix = get_distance_matrix(cat_labels=cat_labels, encoder=train_dataset.label_encoder, power=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fd1e4f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarhical_cross_entropy(input, target):\n",
    "    log_prob = -1.0 * F.log_softmax(input*distance_matrix[target], 1)\n",
    "    loss = log_prob.gather(1, target.unsqueeze(1))\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "def fit_epoch(model, train_loader, criterion, optimizer, sheduler, device: str = 'cpu'):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_data = 0\n",
    "  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(torch.device(device))\n",
    "        labels = labels.to(torch.device(device))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        #target = F.one_hot(labels, 1231).float()\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_data += inputs.size(0)\n",
    "    sheduler.step()\n",
    "    train_loss = running_loss / processed_data\n",
    "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
    "    return train_loss, train_acc\n",
    "\n",
    "def eval_epoch(model, val_loader, criterion, device: str = 'cpu'):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(torch.device(device))\n",
    "        labels = labels.to(torch.device(device))\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            #target = F.one_hot(labels, 1231).float()\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_size += inputs.size(0)\n",
    "    val_loss = running_loss / processed_size\n",
    "    val_acc = running_corrects.double() / processed_size\n",
    "    val_preds = []\n",
    "    val_labels = []\n",
    "    return val_loss, val_acc\n",
    "\n",
    "def train(train_dataset, \n",
    "          val_dataset, \n",
    "          model, epochs, \n",
    "          batch_size, \n",
    "          num_workers=0, \n",
    "          lr: float = 0.01, lr_mult: float = 0.1,\n",
    "          weight_decay=0.0):\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
    "\n",
    "    with tqdm.tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "        gamma = lr_mult ** (2/epochs)\n",
    "        sheduler = lr_scheduler.StepLR(opt, step_size=2, gamma=gamma, verbose=True)\n",
    "        #criterion = nn.CrossEntropyLoss()\n",
    "        criterion = hierarhical_cross_entropy\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt, sheduler)\n",
    "            print(\"loss\", train_loss)\n",
    "            \n",
    "            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
    "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "35fbfa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, test_loader, device: str = 'cpu'):\n",
    "    with torch.no_grad():\n",
    "        logits = []\n",
    "    \n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(torch.device(device))\n",
    "            model.eval()\n",
    "            outputs = model(inputs).cpu()\n",
    "            logits.append(outputs)\n",
    "            \n",
    "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fefb7fe3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will classify :1231\n",
      "Fcnn(\n",
      "  (out): Linear(in_features=45, out_features=1231, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(data.category_id.values))\n",
    "simple_cnn = Fcnn(n_classes=n_classes, \n",
    "                  emb_dim=train_dataset.dim, \n",
    "                  hidden_dim=2048, dropout=0.3).to(torch.device(\"cpu\"))\n",
    "\n",
    "print(\"we will classify :{}\".format(n_classes))\n",
    "print(simple_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c09b6f7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "epoch:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.5000e-02.\n",
      "Adjusting learning rate of group 0 to 1.5000e-02.\n",
      "loss 0.007412626490913459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  10%|█         | 1/10 [01:03<09:34, 63.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 001 train_loss: 0.0074     val_loss 0.3093 train_acc 0.9774 val_acc 0.8530\n",
      "Adjusting learning rate of group 0 to 5.9716e-03.\n",
      "loss 0.00690826336192386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  20%|██        | 2/10 [01:55<07:32, 56.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 002 train_loss: 0.0069     val_loss 0.3041 train_acc 0.9771 val_acc 0.8575\n",
      "Adjusting learning rate of group 0 to 5.9716e-03.\n",
      "loss 0.006287892233859151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  30%|███       | 3/10 [02:45<06:15, 53.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 003 train_loss: 0.0063     val_loss 0.3045 train_acc 0.9780 val_acc 0.8585\n",
      "Adjusting learning rate of group 0 to 2.3773e-03.\n",
      "loss 0.006100818360407995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  40%|████      | 4/10 [03:48<05:43, 57.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 004 train_loss: 0.0061     val_loss 0.3046 train_acc 0.9795 val_acc 0.8585\n",
      "Adjusting learning rate of group 0 to 2.3773e-03.\n",
      "loss 0.0060397277149275555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  50%|█████     | 5/10 [04:55<05:03, 60.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 005 train_loss: 0.0060     val_loss 0.3038 train_acc 0.9798 val_acc 0.8605\n",
      "Adjusting learning rate of group 0 to 9.4644e-04.\n",
      "loss 0.006020331431680154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  60%|██████    | 6/10 [06:02<04:12, 63.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 006 train_loss: 0.0060     val_loss 0.3033 train_acc 0.9799 val_acc 0.8602\n",
      "Adjusting learning rate of group 0 to 9.4644e-04.\n",
      "loss 0.005965916720878712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  70%|███████   | 7/10 [07:09<03:13, 64.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 007 train_loss: 0.0060     val_loss 0.3031 train_acc 0.9799 val_acc 0.8608\n",
      "Adjusting learning rate of group 0 to 3.7678e-04.\n",
      "loss 0.005965177740283898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  80%|████████  | 8/10 [08:17<02:10, 65.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 008 train_loss: 0.0060     val_loss 0.3031 train_acc 0.9797 val_acc 0.8605\n",
      "Adjusting learning rate of group 0 to 3.7678e-04.\n",
      "loss 0.005941477542317348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  90%|█████████ | 9/10 [09:26<01:06, 66.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 009 train_loss: 0.0059     val_loss 0.3029 train_acc 0.9799 val_acc 0.8608\n",
      "Adjusting learning rate of group 0 to 1.5000e-04.\n",
      "loss 0.005941414092445585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 100%|██████████| 10/10 [10:34<00:00, 63.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 010 train_loss: 0.0059     val_loss 0.3029 train_acc 0.9799 val_acc 0.8608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "history = train(train_dataset, \n",
    "                 valid_dataset, \n",
    "                 model=simple_cnn, \n",
    "                 epochs=10, \n",
    "                 batch_size=18630, \n",
    "                 num_workers=0, \n",
    "                 lr=0.015, lr_mult = 0.01, \n",
    "                 weight_decay=0.3) \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847914e3",
   "metadata": {},
   "source": [
    "### Тестирование модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "bc396b49",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_dataset = KEdataset(data=data_valid, \n",
    "                         encoders=encoders, \n",
    "                         document_col='Document', \n",
    "                         category_col='category_id', \n",
    "                         mode='test', \n",
    "                         text_processor=text_processor,\n",
    "                         label_encoder=train_dataset.label_encoder, \n",
    "                         simple_lemms=True)\n",
    "val_preds = predict(simple_cnn, \n",
    "                    DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "df7c3d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation hF1=0.916\n"
     ]
    }
   ],
   "source": [
    "pred_torch_valid = list(train_dataset.label_encoder.inverse_transform(val_preds.argmax(axis=1)))\n",
    "print(f'Validation hF1={cat_tree.hF1_score(valid_target, pred_torch_valid):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d706d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663e5bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2b4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
